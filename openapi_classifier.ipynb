{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "openapi-classifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/pmanlukas/colab/blob/master/openapi_classifier.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "RlJFXtFWandL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# openAPI spec classifier\n",
        "This notebook is used to create a first proof of concept on a classifier for openAPI files. The classifier will be based on a neural network based architecture and will be implemented in Tensorflow, SciPy and Keras."
      ]
    },
    {
      "metadata": {
        "id": "sp4kSQncaqEY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2278
        },
        "outputId": "c2f62871-64d3-4eea-e4db-6146122a56c7"
      },
      "cell_type": "code",
      "source": [
        "# Install a Drive FUSE wrapper.\n",
        "# https://github.com/astrada/google-drive-ocamlfuse\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package cron.\n",
            "(Reading database ... 18298 files and directories currently installed.)\n",
            "Preparing to unpack .../00-cron_3.0pl1-128ubuntu5_amd64.deb ...\n",
            "Unpacking cron (3.0pl1-128ubuntu5) ...\n",
            "Selecting previously unselected package libapparmor1:amd64.\n",
            "Preparing to unpack .../01-libapparmor1_2.11.0-2ubuntu17.1_amd64.deb ...\n",
            "Unpacking libapparmor1:amd64 (2.11.0-2ubuntu17.1) ...\n",
            "Selecting previously unselected package libdbus-1-3:amd64.\n",
            "Preparing to unpack .../02-libdbus-1-3_1.10.22-1ubuntu1_amd64.deb ...\n",
            "Unpacking libdbus-1-3:amd64 (1.10.22-1ubuntu1) ...\n",
            "Selecting previously unselected package dbus.\n",
            "Preparing to unpack .../03-dbus_1.10.22-1ubuntu1_amd64.deb ...\n",
            "Unpacking dbus (1.10.22-1ubuntu1) ...\n",
            "Selecting previously unselected package dirmngr.\n",
            "Preparing to unpack .../04-dirmngr_2.1.15-1ubuntu8_amd64.deb ...\n",
            "Unpacking dirmngr (2.1.15-1ubuntu8) ...\n",
            "Selecting previously unselected package distro-info-data.\n",
            "Preparing to unpack .../05-distro-info-data_0.36ubuntu0.2_all.deb ...\n",
            "Unpacking distro-info-data (0.36ubuntu0.2) ...\n",
            "Selecting previously unselected package libkmod2:amd64.\n",
            "Preparing to unpack .../06-libkmod2_24-1ubuntu2_amd64.deb ...\n",
            "Unpacking libkmod2:amd64 (24-1ubuntu2) ...\n",
            "Selecting previously unselected package kmod.\n",
            "Preparing to unpack .../07-kmod_24-1ubuntu2_amd64.deb ...\n",
            "Unpacking kmod (24-1ubuntu2) ...\n",
            "Selecting previously unselected package lsb-release.\n",
            "Preparing to unpack .../08-lsb-release_9.20160110ubuntu5_all.deb ...\n",
            "Unpacking lsb-release (9.20160110ubuntu5) ...\n",
            "Selecting previously unselected package libgirepository-1.0-1:amd64.\n",
            "Preparing to unpack .../09-libgirepository-1.0-1_1.54.1-1_amd64.deb ...\n",
            "Unpacking libgirepository-1.0-1:amd64 (1.54.1-1) ...\n",
            "Selecting previously unselected package gir1.2-glib-2.0:amd64.\n",
            "Preparing to unpack .../10-gir1.2-glib-2.0_1.54.1-1_amd64.deb ...\n",
            "Unpacking gir1.2-glib-2.0:amd64 (1.54.1-1) ...\n",
            "Selecting previously unselected package iso-codes.\n",
            "Preparing to unpack .../11-iso-codes_3.75-1_all.deb ...\n",
            "Unpacking iso-codes (3.75-1) ...\n",
            "Selecting previously unselected package libdbus-glib-1-2:amd64.\n",
            "Preparing to unpack .../12-libdbus-glib-1-2_0.108-2_amd64.deb ...\n",
            "Unpacking libdbus-glib-1-2:amd64 (0.108-2) ...\n",
            "Selecting previously unselected package python-apt-common.\n",
            "Preparing to unpack .../13-python-apt-common_1.4.0~beta3build2_all.deb ...\n",
            "Unpacking python-apt-common (1.4.0~beta3build2) ...\n",
            "Selecting previously unselected package python3-apt.\n",
            "Preparing to unpack .../14-python3-apt_1.4.0~beta3build2_amd64.deb ...\n",
            "Unpacking python3-apt (1.4.0~beta3build2) ...\n",
            "Selecting previously unselected package python3-dbus.\n",
            "Preparing to unpack .../15-python3-dbus_1.2.4-1build3_amd64.deb ...\n",
            "Unpacking python3-dbus (1.2.4-1build3) ...\n",
            "Selecting previously unselected package python3-gi.\n",
            "Preparing to unpack .../16-python3-gi_3.24.1-2build1_amd64.deb ...\n",
            "Unpacking python3-gi (3.24.1-2build1) ...\n",
            "Selecting previously unselected package module-init-tools.\n",
            "Preparing to unpack .../17-module-init-tools_24-1ubuntu2_all.deb ...\n",
            "Unpacking module-init-tools (24-1ubuntu2) ...\n",
            "Selecting previously unselected package python-apt.\n",
            "Preparing to unpack .../18-python-apt_1.4.0~beta3build2_amd64.deb ...\n",
            "Unpacking python-apt (1.4.0~beta3build2) ...\n",
            "Selecting previously unselected package python-pycurl.\n",
            "Preparing to unpack .../19-python-pycurl_7.43.0-2build2_amd64.deb ...\n",
            "Unpacking python-pycurl (7.43.0-2build2) ...\n",
            "Selecting previously unselected package python-software-properties.\n",
            "Preparing to unpack .../20-python-software-properties_0.96.24.17_all.deb ...\n",
            "Unpacking python-software-properties (0.96.24.17) ...\n",
            "Selecting previously unselected package python3-software-properties.\n",
            "Preparing to unpack .../21-python3-software-properties_0.96.24.17_all.deb ...\n",
            "Unpacking python3-software-properties (0.96.24.17) ...\n",
            "Selecting previously unselected package software-properties-common.\n",
            "Preparing to unpack .../22-software-properties-common_0.96.24.17_all.deb ...\n",
            "Unpacking software-properties-common (0.96.24.17) ...\n",
            "Selecting previously unselected package unattended-upgrades.\n",
            "Preparing to unpack .../23-unattended-upgrades_0.98ubuntu1.1_all.deb ...\n",
            "Unpacking unattended-upgrades (0.98ubuntu1.1) ...\n",
            "Setting up python-apt-common (1.4.0~beta3build2) ...\n",
            "Setting up python3-apt (1.4.0~beta3build2) ...\n",
            "Setting up iso-codes (3.75-1) ...\n",
            "Setting up distro-info-data (0.36ubuntu0.2) ...\n",
            "Setting up python-pycurl (7.43.0-2build2) ...\n",
            "Setting up lsb-release (9.20160110ubuntu5) ...\n",
            "Setting up libgirepository-1.0-1:amd64 (1.54.1-1) ...\n",
            "Setting up libkmod2:amd64 (24-1ubuntu2) ...\n",
            "Setting up gir1.2-glib-2.0:amd64 (1.54.1-1) ...\n",
            "Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n",
            "Setting up libapparmor1:amd64 (2.11.0-2ubuntu17.1) ...\n",
            "Setting up unattended-upgrades (0.98ubuntu1.1) ...\n",
            "\n",
            "Creating config file /etc/apt/apt.conf.d/20auto-upgrades with new version\n",
            "\n",
            "Creating config file /etc/apt/apt.conf.d/50unattended-upgrades with new version\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "invoke-rc.d: could not determine current runlevel\r\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up dirmngr (2.1.15-1ubuntu8) ...\n",
            "Setting up cron (3.0pl1-128ubuntu5) ...\n",
            "Adding group `crontab' (GID 102) ...\n",
            "Done.\n",
            "update-rc.d: warning: start and stop actions are no longer supported; falling back to defaults\n",
            "update-rc.d: warning: stop runlevel arguments (1) do not match cron Default-Stop values (none)\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libdbus-1-3:amd64 (1.10.22-1ubuntu1) ...\n",
            "Setting up kmod (24-1ubuntu2) ...\n",
            "Setting up libdbus-glib-1-2:amd64 (0.108-2) ...\n",
            "Setting up python3-gi (3.24.1-2build1) ...\n",
            "Setting up module-init-tools (24-1ubuntu2) ...\n",
            "Setting up python3-software-properties (0.96.24.17) ...\n",
            "Setting up dbus (1.10.22-1ubuntu1) ...\n",
            "Setting up python-apt (1.4.0~beta3build2) ...\n",
            "Setting up python3-dbus (1.2.4-1build3) ...\n",
            "Setting up python-software-properties (0.96.24.17) ...\n",
            "Setting up software-properties-common (0.96.24.17) ...\n",
            "Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n",
            "Processing triggers for dbus (1.10.22-1ubuntu1) ...\n",
            "gpg: keybox '/tmp/tmp12rx1edp/pubring.gpg' created\n",
            "gpg: /tmp/tmp12rx1edp/trustdb.gpg: trustdb created\n",
            "gpg: key AD5F235DF639B041: public key \"Launchpad PPA for Alessandro Strada\" imported\n",
            "gpg: Total number processed: 1\n",
            "gpg:               imported: 1\n",
            "Warning: apt-key output should not be parsed (stdout is not a terminal)\n",
            "Selecting previously unselected package libfuse2:amd64.\n",
            "(Reading database ... 19706 files and directories currently installed.)\n",
            "Preparing to unpack .../libfuse2_2.9.7-1ubuntu1_amd64.deb ...\n",
            "Unpacking libfuse2:amd64 (2.9.7-1ubuntu1) ...\n",
            "Selecting previously unselected package fuse.\n",
            "Preparing to unpack .../fuse_2.9.7-1ubuntu1_amd64.deb ...\n",
            "Unpacking fuse (2.9.7-1ubuntu1) ...\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.6.21-0ubuntu2_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.6.21-0ubuntu2) ...\n",
            "Setting up libfuse2:amd64 (2.9.7-1ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n",
            "Setting up fuse (2.9.7-1ubuntu1) ...\n",
            "Setting up google-drive-ocamlfuse (0.6.21-0ubuntu2) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5W9g2Zp9a45h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Generate auth tokens for Colab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b2uG7hEra8k9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "9fcfc45f-5800-4a54-c1b4-d85c160f51e5"
      },
      "cell_type": "code",
      "source": [
        "# Generate creds for the Drive FUSE library.\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\r\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "63mVwUIrbEwM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a directory and mount Google Drive using that directory.\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-hOy5JgWbF9t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a folder to use for the project data\n",
        "!mkdir -p /content/drive/openapi-data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kVcmQOfWbII1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2deeceb2-a1b1-41fd-802f-333434e02d96"
      },
      "cell_type": "code",
      "source": [
        "!ls /content/drive/openapi-data/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mnist.py  path_labels.csv  pickle  structure_labels.csv\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uiZtaLU9bLc4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1789e60f-b30f-47e8-ee58-d25c9c55ab47"
      },
      "cell_type": "code",
      "source": [
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "jnC7eg1VbQxv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "daadb27a-2013-44eb-c0c4-9cc225f1cc0d"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "DWgO0pzBbTec",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "4a55fdf7-1e87-4a96-a575-643f37bf7d00"
      },
      "cell_type": "code",
      "source": [
        "!apt-get -qq install -y libarchive-dev && pip install -q -U libarchive\n",
        "import libarchive"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package liblzo2-2:amd64.\n",
            "(Reading database ... 19735 files and directories currently installed.)\n",
            "Preparing to unpack .../liblzo2-2_2.08-1.2_amd64.deb ...\n",
            "Unpacking liblzo2-2:amd64 (2.08-1.2) ...\n",
            "Selecting previously unselected package libarchive13:amd64.\n",
            "Preparing to unpack .../libarchive13_3.2.2-3.1_amd64.deb ...\n",
            "Unpacking libarchive13:amd64 (3.2.2-3.1) ...\n",
            "Selecting previously unselected package libarchive-dev:amd64.\n",
            "Preparing to unpack .../libarchive-dev_3.2.2-3.1_amd64.deb ...\n",
            "Unpacking libarchive-dev:amd64 (3.2.2-3.1) ...\n",
            "Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n",
            "Setting up liblzo2-2:amd64 (2.08-1.2) ...\n",
            "Setting up libarchive13:amd64 (3.2.2-3.1) ...\n",
            "Setting up libarchive-dev:amd64 (3.2.2-3.1) ...\n",
            "Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XymyVRyfbYv1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"drive/openapi-data\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E6hH0iysbZvS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "36d0cfdc-eb77-42a9-82e4-a7ebd91ebda6"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mnist.py  path_labels.csv  pickle  structure_labels.csv\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hceoBAYuboKP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GCgYSYzEbc3o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_data = dict()\n",
        "\n",
        "with open('pickle/specs.pkl', 'rb') as handle:\n",
        "    training_data = pickle.load(handle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lhrdZLcvb6Zy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "keys = list(training_data.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jcs3abVAcBog",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "473c8953-43e5-440d-ccdd-88fccd7e13bd"
      },
      "cell_type": "code",
      "source": [
        "training_data[keys[0]]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'basePath': '/forex-quotes',\n",
              " 'host': '1forge.com',\n",
              " 'info': {'contact': {'email': 'contact@1forge.com',\n",
              "   'name': '1Forge',\n",
              "   'url': 'http://1forge.com'},\n",
              "  'description': 'Stock and Forex Data and Realtime Quotes',\n",
              "  'title': '1Forge Finance APIs',\n",
              "  'version': '0.0.1',\n",
              "  'x-apisguru-categories': ['financial'],\n",
              "  'x-logo': {'backgroundColor': '#24292e',\n",
              "   'url': 'https://api.apis.guru/v2/cache/logo/http_1forge.com_logo.png'},\n",
              "  'x-origin': [{'format': 'swagger',\n",
              "    'url': 'http://1forge.com/openapi.json',\n",
              "    'version': '2.0'}],\n",
              "  'x-preferred': True,\n",
              "  'x-providerName': '1forge.com'},\n",
              " 'paths': {'/quotes': {'get': {'description': 'Get quotes',\n",
              "    'externalDocs': {'description': 'Find out more',\n",
              "     'url': 'http://1forge.com/forex-data-api'},\n",
              "    'responses': {'200': {'description': 'A list of quotes'}},\n",
              "    'summary': 'Get quotes for all symbols',\n",
              "    'tags': ['forex', 'finance', 'quotes']}},\n",
              "  '/symbols': {'get': {'description': 'Symbol List',\n",
              "    'externalDocs': {'description': 'Find out more',\n",
              "     'url': 'http://1forge.com/forex-data-api'},\n",
              "    'responses': {'200': {'description': 'A list of symbols',\n",
              "      'schema': {'example': ['EURUSD', 'GBPJPY', 'AUDUSD'],\n",
              "       'items': {'type': 'string'},\n",
              "       'type': 'array'}}},\n",
              "    'summary': 'Get a list of symbols for which we provide real-time quotes',\n",
              "    'tags': ['forex', 'finance', 'quotes']}}},\n",
              " 'produces': ['application/json'],\n",
              " 'schemes': ['https', 'http'],\n",
              " 'swagger': '2.0'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "OF9qnLugK4Zm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from datetime import datetime\n",
        " \n",
        "import json\n",
        " \n",
        "from keras.layers import Embedding, LSTM, Dense, Conv1D, MaxPooling1D, Dropout, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CWOUwgz9c8mM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2bc00a66-cb5c-4496-c05e-f22db413eca0"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mnist.py  path_labels.csv  pickle  structure_labels.csv\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "138mgUsucOaE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labelsP = pd.read_csv(\"path_labels.csv\").values\n",
        "labelsS  = pd.read_csv(\"structure_labels.csv\").values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CsYV2NxPdGYS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a4a04003-379c-4ac8-ebd7-af9810d1f0b8"
      },
      "cell_type": "code",
      "source": [
        "labelsS.shape\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1016, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "Q0_Ij__DLqc2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "971ee9ac-c3ff-4d81-8440-12584eb01009"
      },
      "cell_type": "code",
      "source": [
        "labelsS"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['1forge.com', 1],\n",
              "       ['6-dot-authentiqio.appspot.com', 1],\n",
              "       ['adafruit.com', 2],\n",
              "       ...,\n",
              "       ['zoom.us', 4],\n",
              "       ['zoomconnect.com', 3],\n",
              "       ['zuora.com', 3]], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "p9Rqzzfe1jte",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## test word embeddings"
      ]
    },
    {
      "metadata": {
        "id": "4nV7IzHG2CLC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3848b803-fc43-4d63-cec8-35a39dd114f2"
      },
      "cell_type": "code",
      "source": [
        "texts = []   # list of text samples\n",
        "for spec in training_data:\n",
        "  strJson = json.dumps(training_data[spec])\n",
        "  texts.append(strJson)\n",
        "\n",
        "print(str(len(texts)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wZCUCyYu2LAY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "5d05b0ca-2416-4479-ca64-69628d451928"
      },
      "cell_type": "code",
      "source": [
        "print(type(texts[0]))\n",
        "print(texts[0])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'str'>\n",
            "{\"swagger\": \"2.0\", \"schemes\": [\"https\", \"http\"], \"host\": \"1forge.com\", \"basePath\": \"/forex-quotes\", \"info\": {\"contact\": {\"email\": \"contact@1forge.com\", \"name\": \"1Forge\", \"url\": \"http://1forge.com\"}, \"description\": \"Stock and Forex Data and Realtime Quotes\", \"title\": \"1Forge Finance APIs\", \"version\": \"0.0.1\", \"x-apisguru-categories\": [\"financial\"], \"x-logo\": {\"backgroundColor\": \"#24292e\", \"url\": \"https://api.apis.guru/v2/cache/logo/http_1forge.com_logo.png\"}, \"x-origin\": [{\"format\": \"swagger\", \"url\": \"http://1forge.com/openapi.json\", \"version\": \"2.0\"}], \"x-preferred\": true, \"x-providerName\": \"1forge.com\"}, \"produces\": [\"application/json\"], \"paths\": {\"/quotes\": {\"get\": {\"description\": \"Get quotes\", \"externalDocs\": {\"description\": \"Find out more\", \"url\": \"http://1forge.com/forex-data-api\"}, \"responses\": {\"200\": {\"description\": \"A list of quotes\"}}, \"summary\": \"Get quotes for all symbols\", \"tags\": [\"forex\", \"finance\", \"quotes\"]}}, \"/symbols\": {\"get\": {\"description\": \"Symbol List\", \"externalDocs\": {\"description\": \"Find out more\", \"url\": \"http://1forge.com/forex-data-api\"}, \"responses\": {\"200\": {\"description\": \"A list of symbols\", \"schema\": {\"example\": [\"EURUSD\", \"GBPJPY\", \"AUDUSD\"], \"items\": {\"type\": \"string\"}, \"type\": \"array\"}}}, \"summary\": \"Get a list of symbols for which we provide real-time quotes\", \"tags\": [\"forex\", \"finance\", \"quotes\"]}}}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MYP4cwna1reG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17051
        },
        "outputId": "00e1e4c2-6d65-4868-8e3e-8b61456fadce"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "labels = labelsStruct  # list of label ids\n",
        "print(type(labels))\n",
        "labels"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 3,\n",
              " 2,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 2,\n",
              " 1,\n",
              " 3,\n",
              " 5,\n",
              " 1,\n",
              " 4,\n",
              " 1,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 4,\n",
              " 4,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 1,\n",
              " 4,\n",
              " 1,\n",
              " 1,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 1,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 4,\n",
              " 4,\n",
              " 3,\n",
              " 1,\n",
              " 4,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 4,\n",
              " 3,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 2,\n",
              " 3,\n",
              " 3,\n",
              " 2,\n",
              " 3,\n",
              " 5,\n",
              " 4,\n",
              " 3,\n",
              " 2,\n",
              " 1,\n",
              " 4,\n",
              " 4,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 4,\n",
              " 4,\n",
              " 2,\n",
              " 3,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 6,\n",
              " 1,\n",
              " 4,\n",
              " 5,\n",
              " 2,\n",
              " 1,\n",
              " 5,\n",
              " 3,\n",
              " 1,\n",
              " 3,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 4,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 4,\n",
              " 3,\n",
              " 4,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 3,\n",
              " 5,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 3,\n",
              " 4,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 3,\n",
              " 2,\n",
              " 3,\n",
              " 5,\n",
              " 3,\n",
              " 3,\n",
              " 4,\n",
              " 2,\n",
              " 4,\n",
              " 1,\n",
              " 4,\n",
              " 2,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 4,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 4,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 4,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 3,\n",
              " 1,\n",
              " 4,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 3,\n",
              " 3,\n",
              " 2,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 4,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 4,\n",
              " 2,\n",
              " 3,\n",
              " 3,\n",
              " 2,\n",
              " 3,\n",
              " 1,\n",
              " 5,\n",
              " 3,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 3,\n",
              " 4,\n",
              " 3,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 4,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 4,\n",
              " 4,\n",
              " 2,\n",
              " 4,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 4,\n",
              " 4,\n",
              " 3,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 3,\n",
              " 3,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 1,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 4,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 4,\n",
              " 1,\n",
              " 2,\n",
              " 5,\n",
              " 1,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 3,\n",
              " 1,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 3,\n",
              " 5,\n",
              " 3,\n",
              " 4,\n",
              " 6,\n",
              " 3,\n",
              " 1,\n",
              " 3,\n",
              " 6,\n",
              " 2,\n",
              " 5,\n",
              " 2,\n",
              " 2,\n",
              " 5,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 3,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 5,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 2,\n",
              " 2,\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "metadata": {
        "id": "H7NZ5Egg3xi-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "635282ba-685d-4517-ebd9-00d1761f9021"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
        "\n",
        "encoder = LabelBinarizer()\n",
        "encoder.fit(labels)\n",
        "y_train = encoder.transform(labels)\n",
        "\n",
        "y_train"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, 1, 0, 0],\n",
              "       [0, 0, 1, 0, 0, 0],\n",
              "       [0, 0, 1, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "metadata": {
        "id": "jVX8oniC5Ghv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "from keras.models import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sd4SQgGG-xhz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH = 50000\n",
        "MAX_NUM_WORDS = 20000\n",
        "EMBEDDING_DIM = 100\n",
        "VALIDATION_SPLIT = 0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qnCr_W4U1mxF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "71a1b056-0ee3-4427-8b1a-04af5d34de4c"
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(nb_words=MAX_NUM_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "labels = y_train #keras.utils.to_categorical(np.asarray(labels))\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "# split the data into a training set and a validation set\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
        "\n",
        "x_train = data[:-nb_validation_samples]\n",
        "y_train = labels[:-nb_validation_samples]\n",
        "x_val = data[-nb_validation_samples:]\n",
        "y_val = labels[-nb_validation_samples:]"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/preprocessing/text.py:172: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
            "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 146890 unique tokens.\n",
            "Shape of data tensor: (1016, 50000)\n",
            "Shape of label tensor: (813, 6)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-f94d57966071>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mnb_validation_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVALIDATION_SPLIT\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 963 is out of bounds for axis 0 with size 813"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ibMwkGbw4asi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "b3c79f47-9109-4129-ec0f-988aed4ce2ec"
      },
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 1],\n",
              "       [0, 1, 0, 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 1, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 0, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "metadata": {
        "id": "ze9XE_Nc5J_w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a638d45d-e71b-488f-e21e-698b8f5205b6"
      },
      "cell_type": "code",
      "source": [
        "embeddings_index = {}\n",
        "f = open(os.path.join('wordvector', 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NlCDFnCTCvDS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# prepare embedding matrix\n",
        "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i >= MAX_NUM_WORDS:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sz0BS_PX6rnS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TdJa6cSZ-lHV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "embedding_layer = Embedding(num_words,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6xuW_Z_HC63U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bBgq6WtM-l5S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "3f244d86-d0d2-42d4-c815-796e1a9e866d"
      },
      "cell_type": "code",
      "source": [
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
        "x = MaxPooling1D(5)(x)\n",
        "x = Conv1D(128, 5, activation='relu')(x)\n",
        "x = MaxPooling1D(5)(x)\n",
        "x = Conv1D(128, 5, activation='relu')(x)\n",
        "x = MaxPooling1D(35)(x)  # global max pooling\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "preds = Dense(6, activation='softmax')(x)\n",
        "\n",
        "model = Model(sequence_input, preds)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "# happy learning!\n",
        "\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=10,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.2)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 650 samples, validate on 163 samples\n",
            "Epoch 1/10\n",
            "650/650 [==============================] - 15s 23ms/step - loss: 2.0250 - acc: 0.2815 - val_loss: 1.6708 - val_acc: 0.3436\n",
            "Epoch 2/10\n",
            "650/650 [==============================] - 15s 23ms/step - loss: 1.5747 - acc: 0.3708 - val_loss: 1.3968 - val_acc: 0.4049\n",
            "Epoch 3/10\n",
            "650/650 [==============================] - 15s 23ms/step - loss: 1.3209 - acc: 0.4908 - val_loss: 1.0825 - val_acc: 0.5583\n",
            "Epoch 4/10\n",
            "650/650 [==============================] - 15s 23ms/step - loss: 0.9313 - acc: 0.6031 - val_loss: 1.0650 - val_acc: 0.5890\n",
            "Epoch 5/10\n",
            "650/650 [==============================] - 15s 23ms/step - loss: 0.7822 - acc: 0.6862 - val_loss: 0.9495 - val_acc: 0.7178\n",
            "Epoch 6/10\n",
            "650/650 [==============================] - 15s 23ms/step - loss: 0.6599 - acc: 0.7446 - val_loss: 0.8790 - val_acc: 0.6380\n",
            "Epoch 7/10\n",
            "650/650 [==============================] - 15s 23ms/step - loss: 0.6523 - acc: 0.7677 - val_loss: 0.7652 - val_acc: 0.7362\n",
            "Epoch 8/10\n",
            "480/650 [=====================>........] - ETA: 3s - loss: 0.5095 - acc: 0.8063"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "650/650 [==============================] - 15s 23ms/step - loss: 0.4989 - acc: 0.8092 - val_loss: 0.8037 - val_acc: 0.7239\n",
            "Epoch 9/10\n",
            "650/650 [==============================] - 15s 23ms/step - loss: 0.4275 - acc: 0.8369 - val_loss: 1.0115 - val_acc: 0.6748\n",
            "Epoch 10/10\n",
            "650/650 [==============================] - 15s 23ms/step - loss: 0.3756 - acc: 0.8523 - val_loss: 0.9227 - val_acc: 0.7669\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eDXv4-zM_FhB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "77be8ce1-1c1f-4f15-9401-153f45ac493a"
      },
      "cell_type": "code",
      "source": [
        "score = model.evaluate(x_val, y_val,\n",
        "                       batch_size=32, verbose=1)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "203/203 [==============================] - 2s 8ms/step\n",
            "Test score: 0.9178235795110318\n",
            "Test accuracy: 0.7832512327015694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZWaParwiNHPz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prepare Labels and Specs for the model\n",
        "In this part of the notebook, we will create three list. One contains the specs and the other two the labels for paths and for specs. The lists are later used as input for the actual preprocessing and then later the model."
      ]
    },
    {
      "metadata": {
        "id": "mk4pnqauKBXl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "305b8f91-0c22-4b89-88a0-3fd211fb8a37"
      },
      "cell_type": "code",
      "source": [
        "labelsPath = list()\n",
        "labelsStruct = list()\n",
        "\n",
        "\n",
        "for labs in labelsS:\n",
        "  labelsStruct.append(labs[1])\n",
        "\n",
        "for labp in labelsP:\n",
        "  labelsPath.append(labp[1])\n",
        "  \n",
        "print(str(len(labelsPath)))\n",
        "print(str(len(labelsStruct)))\n",
        "\n",
        "#labelsStruct = labelsS[\"cluster\"].tolist()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1016\n",
            "1016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hHAZOfoPMOFl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "4602c1ac-c9c9-44d5-f5aa-3c1d7d8c5029"
      },
      "cell_type": "code",
      "source": [
        "print(labelsPath)\n",
        "print(labelsStruct)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 4, 4, 8, 3, 6, 8, 3, 4, 8, 6, 2, 4, 4, 2, 8, 6, 3, 4, 4, 8, 4, 9, 4, 4, 3, 3, 3, 4, 4, 6, 4, 8, 3, 9, 4, 4, 3, 3, 4, 3, 3, 3, 10, 4, 3, 3, 3, 8, 3, 3, 3, 4, 6, 1, 8, 4, 8, 6, 4, 6, 3, 3, 2, 6, 2, 3, 6, 9, 9, 4, 5, 1, 4, 8, 3, 4, 3, 6, 3, 3, 5, 4, 8, 8, 8, 9, 9, 8, 1, 4, 8, 3, 6, 4, 3, 3, 2, 9, 6, 6, 3, 8, 2, 3, 4, 5, 1, 6, 3, 8, 8, 6, 4, 8, 8, 4, 3, 4, 6, 4, 6, 2, 8, 4, 3, 9, 1, 6, 6, 3, 3, 4, 2, 3, 2, 6, 6, 4, 7, 1, 7, 7, 1, 7, 7, 7, 7, 7, 5, 1, 7, 7, 7, 7, 7, 7, 1, 7, 7, 7, 7, 7, 7, 7, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 2, 7, 7, 2, 7, 7, 7, 2, 3, 5, 1, 8, 3, 9, 2, 4, 9, 2, 8, 9, 4, 9, 9, 9, 8, 2, 9, 2, 9, 9, 1, 2, 9, 8, 9, 5, 2, 2, 7, 4, 2, 2, 9, 7, 5, 5, 9, 9, 5, 5, 9, 9, 7, 9, 8, 7, 2, 8, 2, 9, 9, 5, 9, 9, 9, 2, 5, 5, 2, 5, 7, 2, 8, 9, 5, 1, 1, 9, 5, 9, 9, 5, 7, 1, 1, 1, 1, 1, 9, 5, 1, 1, 1, 1, 1, 1, 1, 9, 1, 1, 1, 1, 1, 7, 5, 5, 9, 4, 8, 9, 4, 2, 2, 4, 4, 6, 1, 5, 1, 3, 9, 8, 5, 7, 7, 9, 9, 8, 9, 7, 9, 4, 3, 9, 3, 4, 3, 2, 8, 5, 4, 8, 6, 2, 9, 4, 4, 5, 5, 7, 9, 1, 9, 7, 9, 4, 4, 3, 7, 5, 2, 3, 2, 2, 2, 2, 8, 2, 9, 5, 2, 8, 4, 9, 9, 7, 1, 9, 9, 7, 7, 1, 5, 7, 9, 7, 7, 7, 5, 7, 9, 8, 6, 2, 9, 7, 7, 8, 8, 2, 2, 4, 7, 9, 2, 2, 7, 7, 8, 4, 9, 5, 4, 8, 8, 2, 8, 8, 2, 2, 8, 5, 1, 7, 2, 7, 4, 4, 2, 1, 7, 7, 6, 8, 4, 2, 9, 4, 5, 9, 8, 8, 5, 2, 2, 2, 2, 5, 8, 2, 7, 8, 4, 4, 3, 6, 2, 2, 5, 5, 5, 1, 1, 1, 7, 7, 1, 5, 7, 1, 1, 2, 1, 9, 9, 5, 9, 9, 5, 5, 9, 5, 7, 5, 7, 9, 5, 1, 5, 5, 9, 5, 9, 1, 2, 9, 2, 9, 2, 1, 5, 8, 2, 6, 9, 9, 9, 2, 7, 5, 9, 8, 2, 5, 8, 8, 4, 4, 7, 9, 7, 8, 7, 8, 5, 9, 8, 5, 10, 2, 10, 3, 4, 4, 9, 8, 6, 7, 3, 5, 6, 8, 3, 3, 7, 6, 1, 2, 4, 4, 8, 8, 6, 6, 8, 5, 10, 4, 3, 4, 3, 8, 4, 8, 5, 8, 10, 3, 6, 7, 5, 1, 9, 8, 1, 5, 3, 8, 8, 5, 1, 5, 9, 2, 5, 5, 8, 6, 10, 7, 6, 9, 8, 5, 1, 5, 9, 9, 4, 8, 8, 4, 3, 8, 2, 8, 2, 4, 8, 4, 2, 8, 2, 8, 1, 5, 1, 5, 5, 8, 4, 9, 1, 5, 5, 5, 8, 4, 1, 2, 1, 5, 5, 4, 3, 1, 5, 8, 4, 5, 8, 4, 6, 4, 3, 4, 9, 1, 4, 1, 2, 10, 8, 4, 2, 3, 2, 9, 10, 10, 5, 1, 3, 3, 4, 5, 2, 3, 4, 6, 1, 4, 6, 8, 3, 3, 1, 2, 4, 8, 3, 3, 3, 2, 2, 3, 2, 8, 8, 2, 8, 8, 4, 2, 4, 9, 4, 5, 4, 10, 8, 3, 6, 1, 3, 8, 8, 4, 6, 5, 5, 4, 8, 8, 8, 3, 9, 5, 2, 8, 8, 4, 6, 8, 3, 3, 3, 1, 5, 4, 4, 1, 2, 2, 4, 9, 4, 4, 8, 2, 2, 1, 4, 8, 1, 2, 8, 4, 2, 2, 4, 4, 1, 8, 8, 4, 8, 9, 8, 4, 1, 2, 2, 4, 2, 2, 4, 2, 2, 2, 4, 2, 9, 3, 3, 8, 8, 8, 3, 2, 8, 2, 4, 8, 2, 2, 9, 4, 1, 1, 1, 8, 3, 2, 2, 8, 8, 8, 9, 9, 4, 5, 5, 1, 9, 9, 5, 4, 5, 4, 5, 8, 8, 8, 4, 2, 8, 9, 8, 3, 6, 1, 7, 7, 1, 5, 2, 8, 4, 2, 3, 9, 2, 6, 3, 9, 6, 8, 1, 10, 1, 4, 4, 3, 8, 9, 3, 7, 7, 9, 1, 5, 8, 8, 8, 10, 2, 6, 8, 3, 9, 9, 7, 7, 5, 5, 5, 7, 4, 5, 7, 1, 3, 4, 5, 6, 3, 6, 9, 8, 4, 3, 4, 8, 3, 8, 1, 5, 5, 7, 7, 2, 2, 7, 9, 5, 1, 7, 5, 7, 1, 5, 2, 7, 9, 2, 7, 4, 8, 2, 4, 9, 6, 10, 2, 4, 7, 2, 1, 8, 8, 2, 10, 9, 7, 9, 7, 2, 8, 3, 4, 8, 2, 8, 2, 6, 8, 5, 6, 3, 8, 2, 8, 8, 4, 3, 9, 5, 7, 4, 10, 3, 5, 6, 6, 2, 4, 8, 7, 5, 2, 4, 6, 6, 4, 8, 2, 8, 6, 5, 1, 9, 8, 9, 2, 4, 9, 6, 8, 4, 4, 3, 9, 10, 4, 4, 6, 3, 6, 8, 3, 2, 3, 4, 3, 3, 6, 2, 4, 3, 8, 5, 4, 3, 9, 9, 9, 9, 7, 5, 8, 7, 4, 3, 4, 5, 2, 1, 2, 9, 1, 8, 1, 4, 6, 1, 6, 1, 8, 8, 9, 6, 3, 10]\n",
            "[1, 1, 2, 2, 1, 1, 3, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 1, 3, 5, 1, 4, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 1, 1, 2, 3, 1, 4, 1, 1, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 2, 2, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 1, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 4, 4, 3, 1, 4, 1, 1, 1, 4, 3, 1, 2, 3, 3, 3, 2, 3, 3, 2, 3, 5, 4, 3, 2, 1, 4, 4, 2, 1, 1, 4, 4, 2, 3, 1, 2, 2, 6, 1, 4, 5, 2, 1, 5, 3, 1, 3, 1, 5, 1, 4, 1, 5, 1, 4, 3, 4, 3, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 3, 4, 1, 1, 1, 3, 2, 3, 5, 3, 3, 4, 2, 4, 1, 4, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 1, 1, 5, 1, 3, 1, 4, 2, 1, 1, 1, 2, 2, 2, 2, 5, 1, 1, 3, 1, 1, 1, 3, 6, 6, 5, 5, 5, 3, 3, 2, 1, 5, 1, 5, 1, 1, 5, 1, 5, 1, 1, 2, 2, 1, 4, 1, 1, 1, 1, 1, 1, 1, 5, 4, 2, 3, 3, 2, 3, 1, 5, 3, 3, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, 3, 3, 3, 1, 1, 1, 1, 3, 4, 3, 2, 1, 1, 1, 4, 2, 2, 1, 2, 2, 4, 4, 2, 4, 2, 2, 2, 2, 2, 1, 1, 1, 4, 4, 3, 1, 2, 1, 3, 3, 2, 1, 1, 2, 1, 1, 2, 3, 3, 1, 1, 1, 1, 1, 1, 1, 2, 3, 3, 3, 1, 3, 3, 3, 4, 2, 3, 4, 3, 1, 1, 4, 1, 2, 5, 1, 3, 1, 1, 5, 1, 3, 1, 3, 1, 1, 1, 3, 4, 5, 3, 4, 5, 1, 2, 1, 2, 1, 1, 1, 1, 1, 3, 5, 3, 4, 6, 3, 1, 3, 6, 2, 5, 2, 2, 5, 3, 1, 1, 1, 2, 3, 5, 1, 1, 1, 2, 3, 5, 1, 1, 5, 1, 3, 2, 2, 2, 2, 2, 1, 2, 5, 6, 6, 3, 2, 2, 2, 2, 2, 2, 2, 5, 4, 1, 3, 5, 3, 1, 3, 4, 3, 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qdfIykU8_70j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create training and test split of the labels"
      ]
    },
    {
      "metadata": {
        "id": "QV9VUUhm94t5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Y_Path_train = labelsPath[:808]\n",
        "Y_Path_test = labelsPath[808:]\n",
        "\n",
        "Y_Struct_train = labelsStruct[:808]\n",
        "Y_Struct_test = labelsStruct[808:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zLeNZ7ZY-dP1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(len(Y_Path_train))\n",
        "print(len(Y_Path_test))\n",
        "print(len(Y_Struct_train))\n",
        "print(len(Y_Struct_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qwjBHNRu-8gF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('pickle/Y_Path_train.pickle', 'wb') as handle:\n",
        "    pickle.dump(Y_Path_train, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vX0K3n0O-_cl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('pickle/Y_Path_test.pickle', 'wb') as handle:\n",
        "    pickle.dump(Y_Path_test, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2V7kDySY-_fZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('pickle/Y_Struct_train.pickle', 'wb') as handle:\n",
        "    pickle.dump(Y_Struct_train, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KZSX5Ou4-_iM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('pickle/Y_Struct_test.pickle', 'wb') as handle:\n",
        "    pickle.dump(Y_Struct_test, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iR3BDzONNDXh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "specs_json = list()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5EjHsDBSqaxz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hJQsSGq9Ni16",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for spec in training_data:\n",
        "  strJson = json.dumps(training_data[spec])\n",
        "  specs_json.append(strJson)\n",
        "\n",
        "print(str(len(specs_json)))\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2v5Zl4GBOgSD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "specs_json[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WR_SE_dTm4ci",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = specs_json[:808]\n",
        "X_test = specs_json[808:]\n",
        "\n",
        "print(len(X_train))\n",
        "print(len(X_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d7iwYpbNOyir",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preprocessing\n",
        "In this part of the notebook we are going to preprocess the data itself. The data will be tokenized, preprocessed and stored as Training and Test sets"
      ]
    },
    {
      "metadata": {
        "id": "CuGZRu7DqDEX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u1C6_30s9H3F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once fit, the Tokenizer provides 4 attributes that you can use to query what has been learned about your documents:\n",
        "\n",
        "*   word_counts: A dictionary of words and their counts.\n",
        "*   word_docs: A dictionary of words and how many documents each appeared in.\n",
        "*   word_index: A dictionary of words and their uniquely assigned integers.\n",
        "*   document_count:An integer count of the total number of documents that were used to fit the Tokenizer.\n",
        "\n",
        "\n",
        "[source](https://machinelearningmastery.com/prepare-text-data-deep-learning-keras/)\n"
      ]
    },
    {
      "metadata": {
        "id": "kUGDglE0qLI_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=10000,filters='!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "\n",
        "tokenizer\n",
        "\n",
        "tokenizer.fit_on_texts(specs_json)\n",
        "\n",
        "\n",
        "\n",
        "x_train = tokenizer.texts_to_matrix(X_train)\n",
        "x_test = tokenizer.texts_to_matrix(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WsgqtUCeNbRw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "--------------------------------"
      ]
    },
    {
      "metadata": {
        "id": "jz4rsCxiMze_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q7laEC0enX73",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n6Iwh_emNDNx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
        "\n",
        "encoder = LabelBinarizer()\n",
        "encoder.fit(labelsStruct)\n",
        "y_train = encoder.transform(Y_Struct_train)\n",
        "y_test = encoder.transform(Y_Struct_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jMkvycgpN2Xh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zZ7-cvhMNfDq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(x_train[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y6fsswTNNyOH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('y_train shape:', y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VXyYma_oOEub",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('x_train shape:', x_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4s0Ib7kGOI1z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "epochs = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4ybyYVkwNcv2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---------------------------------------------------------------"
      ]
    },
    {
      "metadata": {
        "id": "XcdTm3h8v5cS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Architecture\n",
        "In this section of the notebook the actual deep learning model is designed and the architecture presented before the actual training of it will take place."
      ]
    },
    {
      "metadata": {
        "id": "Jd7dTA9vASip",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(10000, 128, input_length=10000))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(64, 5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=4))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(6, activation='softmax',name = 'softmax_layer'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7RJGHv4wBLtm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CUX2pTzajFYm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(model, to_file='model.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V8Yt5meuCIpu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=3,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C2FJ7cblWJvz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model2 = Sequential()\n",
        "model2.add(Dense(512, input_shape=(10000,)))\n",
        "model2.add(Activation('relu'))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(Dense(512))\n",
        "model2.add(Activation('relu'))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(Dense(512))\n",
        "model2.add(Activation('relu'))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(Dense(512))\n",
        "model2.add(Activation('relu'))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(Dense(512))\n",
        "model2.add(Activation('relu'))\n",
        "model2.add(Dense(6))\n",
        "model2.add(Activation('softmax'))\n",
        "\n",
        "model2.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KQlNbNAGnijL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(model2.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BmhQZZp6jAR4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(model, to_file='model2.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jnUsffQZWRBg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history2 = model2.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=15,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vzUDM9CDCJRT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### first training"
      ]
    },
    {
      "metadata": {
        "id": "9l7QvruIHdqR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score = model.evaluate(x_test, y_test,\n",
        "                       batch_size=batch_size, verbose=1)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ctpjYyyMpEqo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score = model2.evaluate(x_test, y_test,\n",
        "                       batch_size=batch_size, verbose=1)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8WQ4F679hyLm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9vfewUCch1Rt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_and_metrics = model2.evaluate(x_test, y_test, batch_size=128)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8f_DGORah3_e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Use Model"
      ]
    },
    {
      "metadata": {
        "id": "7KJBPAjzh-MK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classes = model.predict(x_test, batch_size=128)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n09BxUwsiFsU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classes2 = model2.predict(x_test, batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "crjp_6xirw1Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "FdbOe8uvr12Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text_labels = encoder.classes_ \n",
        "\n",
        "for i in range(10):\n",
        "    prediction = model.predict(np.array([x_test[i]]))\n",
        "    predicted_label = text_labels[np.argmax(prediction)]\n",
        "    print(test_posts.iloc[i][:50], \"...\")\n",
        "    print('Actual label:' + test_tags.iloc[i])\n",
        "    print(\"Predicted label: \" + predicted_label + \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BWYLiq3Gr4oY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_softmax = model.predict(x_test)\n",
        "\n",
        "y_test_1d = []\n",
        "y_pred_1d = []\n",
        "\n",
        "for i in range(len(y_test)):\n",
        "    probs = y_test[i]\n",
        "    index_arr = np.nonzero(probs)\n",
        "    one_hot_index = index_arr[0].item(0)\n",
        "    y_test_1d.append(one_hot_index)\n",
        "\n",
        "for i in range(0, len(y_softmax)):\n",
        "    probs = y_softmax[i]\n",
        "    predicted_index = np.argmax(probs)\n",
        "    y_pred_1d.append(predicted_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ur-UofB0r8VE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cnf_matrix = confusion_matrix(y_test_1d, y_pred_1d)\n",
        "plt.figure(figsize=(24,20))\n",
        "plot_confusion_matrix(cnf_matrix, classes=text_labels, title=\"Confusion matrix\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}